\documentclass{sig-alternate}
%\documentclass{acm_proc_article-sp}

\usepackage{etex}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{url}
\usepackage{doi}
\usepackage{hyperref}
\usepackage[]{color}
\usepackage{siunitx}
\usepackage[shortlabels]{enumitem}

\definecolor{grey}{rgb}{0.5,0.5,0.5}
\definecolor{darkgreen}{cmyk}{0.7, 0, 1, 0.5}
\definecolor{LinkColor}{rgb}{0,0,0.5}

\setlist{noitemsep}

\hypersetup{%%Hyperref stuff
	pageanchor=false,
	pdfpagelabels=false,
	final,
	colorlinks=true, 
	linkcolor=LinkColor,
	citecolor=LinkColor,
	filecolor=LinkColor,
	menucolor=LinkColor,
	urlcolor=LinkColor,
	pdftitle={The scientific state of the art in crowdsourcing}
}

\begin{document}

\title{The scientific state of the art in crowdsourcing}
\subtitle{An overview of crowdsourcing, its different facets, challenges and criticisms}

\numberofauthors{6}

\author{%% Authors start here
	\alignauthor
	Diego Barbera\\ 
	\affaddr{1327823}
%
	\alignauthor
	Viktor Kvaternjak\\
	\affaddr{1328111}
%
	\alignauthor
	Andreas Ntaflos\\
	\affaddr{0326302}
%
	\and
%
	\alignauthor
	Richard Plangger\\ 
	\affaddr{1025637}
%
	\alignauthor
	Christian Schnitzer\\ 
	\affaddr{0828380}
%
	\alignauthor
	Konrad Steiner\\ 
	\affaddr{0927159}
	\and
%% Authors end here
}

\maketitle

\begin{abstract}
	Crowdsourcing as a practice for outsourcing work to an unidentified public
	is relatively young. In this paper we review current scientific literature
	for an overview of crowdsourcing, focusing on a consistent definition of
	the concept and differentiating between the two main forms of crowdsourcing
	initiatives: those that are creative or intellectual in nature, rewarding
	unique ideas and solutions to problems, and those that involve solving
	so-called micro-tasks that are impossible for a computer but easy for human
	intelligence. These two forms of crowdsourcing involve very different types
	of crowds with different motivations and behaviour. Whereas creative or
	intellectual initiatives involve often rich financial rewards, building of
	communities, practicing special skills and passion for the activity, the
	crowds of micro-task initiatives are anonymous, culturally and
	geographically diverse, and tasks pay only pennies.  These initiatives need
	to be designed in a way that prevents cheating and manipulation while
	ensuring quality of work output. 
\end{abstract}

\section{Introduction}

The term \emph{crowdsourcing} has been coined by Jeff Howe in
2006~\cite{howe2006rise} as a portmanteau of \emph{crowd} and
\emph{outsourcing}. He describes it generally as outsourcing work to an
``undefined, generally large group of people in the form of an open
call''~\cite{howe2009crowdsourcing}. The term and the underlying practice are
still quite young and the theoretical backgrounds---What is a crowd? What
motivates the crowd? Why do some crowdsourcing systems and initiatives thrive
while others fail miserably?---are still being researched, for example in the
works of Daren Brabham~\cite{brabham2008crowdsourcing, brabham2008moving,
	brabham2010moving}. This is why in scientific literature we find various,
often directly conflicting definitions of crowdsourcing, as well as conflicting
classifications of existing systems and initiatives; some authors see Wikipedia
or Youtube as crowdsourcing examples while others argue the exact
opposite~\cite{estelles2012towards}.

In this paper we give an overview of crowdsourcing by discussing an integrated
definition of the term and practice as put forth by Estell{\'e}s-Arolas and
Gonz{\'a}lez-Ladr{\'o}n-de-Guevara in~\cite{estelles2012towards} and examining
current crowdsourcing systems and platforms based on that definition. We also
compare the motivations and composition of different kinds of crowds for
different kinds of problems and how these crowds often differ widely in these
regards. Next we describe broadly how to design a crowdsourcing initiative to be
successful and which steps can be taken to control reliability and quality of
work. Finally we discuss some critical aspects of and problems with
crowdsourcing which are often overlooked or ignored in current literature. 

% What? Term coined by Howe in Wired article howe2006rise
% Various, conflicting definitions: Wikipedia? Youtube? vs InnoCentive, iStockphoto, Amazon Mechanical Turk
% Proper definition by Estell{\'e}s-Arolas and Gonz{\'a}lez-Ladr{\'o}n-de-Guevara
% Purpose of paper:
%  - define what ``crowdsourcing'' really is, based on definition and characteristics by Estelles at al
%  - exemplary overview of crowdsourcing platforms and systems based on definition
%   - what is a CS platform, what isn't?
%  - what it takes to create a successful CS initiative
%  - examination of different types of crowd, incentives, etc and how they apply to different CS initiatives
%  - criticisms of CS

\section{Crowdsourcing defined}\label{csdef}

The most comprehensive definition of crowdsourcing has been compiled by
Estell{\'e}s-Arolas and Gonz{\'a}lez-Ladr{\'o}n-de-Guevara. In their
work~\cite{estelles2012towards} the authors analysed a large number of
existing definitions and extracted various characteristics that apply to a
crowdsourcing system, resulting in the following: 
\begin{quotation}
	Crowdsourcing is a type of participative online activity in which an
	individual, an institution, a non-profit organization, or company proposes to
	a group of individuals of varying knowledge, heterogeneity, and number, via a
	flexible open call, the voluntary undertaking of a task. The undertaking of
	the task, of variable complexity and modularity, and in which the crowd should
	participate bringing their work, money, knowledge and/or experience, always
	entails mutual benefit. The user will receive the satisfaction of a given type
	of need, be it economic, social recognition, self-esteem, or the development
	of individual skills, while the crowdsourcer will obtain and utilize to their
	advantage that what the user has brought to the venture, whose form will
	depend on the type of activity undertaken.
\end{quotation}

Based on that definition all of the following characteristics, as identified
in~\cite{estelles2012towards} apply to a crowdsourcing system:

\begin{enumerate}[(a)]
	\item There is a clearly defined crowd\label{a}
	\item There exists a task with a clear goal\label{b}
	\item The recompense received by the crowd is clear\label{c}
	\item The crowdsourcer is clearly identified\label{d}
	\item The compensation received by the crowdsourcer is clearly defined\label{e}
	\item It is an online assigned process of participative type\label{f}
	\item It uses an open call of variable extent\label{g}
	\item It uses the internet\label{h}
\end{enumerate}

Having identified the characteristics that describe a crowdsourcing system it
is now easy to see that neither Wikipedia, nor Youtube qualify as such. For
Wikipedia the missing characteristics are~\ref{d},~\ref{e} and~\ref{g}, while
for Youtube none except~\ref{a} and~\ref{h} even apply.

The reason crowdsourcing can and often does work is based on the concept of a
``wisdom of crowds'', as defined and discussed by James Surowiecki
in~\cite{surowiecki2004wisdom}.  Based on various case studies Surowiecki
argues that under the right circumstances ``groups can be smarter than the
smartest person in them'' and that the ``wisdom of crowds'' is derived from
aggregating solutions, not by averaging them. Touching upon these insights
Brabham confirms that the internet provides ``the perfect technology'' to
aggregate independent and disparate solutions~\cite{brabham2008crowdsourcing}
conceived by the crowd.

% mostly based on estelles2012towards and brabham2008crowdsourcing
% define what ``crowdsourcing'' really is, based on definition and
%   characteristics by Estelles at al
% concept of ``wisdom of crowds'' and why a crowd may be smarter than its
%   smartest member; average vs aggregation
% distinction from Open Source
% examples of CS vs examples of Not-CS

\section{Overview of Crowdsourcing systems and platforms}
% mostly based on estelles2012towards and hossfeld2013crowdsourcing
% exemplary overview of crowdsourcing platforms and systems based on definition
% what is a CS platform, what isn't?
% CS initiatives run on CS platforms
%   not all initiatives run on all platforms
% hint at topic of different types of crowds for different types of problems
% and tasks

Modern crowdsourcing systems in general can be divided into three different
types, as discussed in~\cite{hossfeld2013crowdtesting}: aggregator platforms,
specialised platforms and crowd provider platforms. 

\emph{Aggregator platforms} such as
Crowdflower\footnote{\url{http://crowdflower.com}} and
Crowdsource\footnote{\url{http://www.crowdsource.com}} primarily focus on
developing crowd-based solutions for businesses that have existing workflows in
need of crowdsourcing. Such platforms do not maintain their own crowd or
workforce but recruit workers from specialised or crowd provider platforms and
thus provide a high level of abstraction from crowdsourcing issues like quality
control and recruiting. 

\emph{Specialised platforms} such as
Threadless\footnote{\url{http://www.threadless.com}},
InnoCentive\footnote{\url{http://www.innocentive.com}} or
Microtask\footnote{\url{http://www.microtask.com}} manage and maintain their
own workforce and focus on very specific tasks, such as t-shirt design, solving
R\&D problems or document processing and data entry. As such the workforce is
usually limited in variety, location, expertise and other aspects.

\emph{Crowd provider platforms} such as Amazon Mechanical
Turk\footnote{\url{http://www.mturk.com}},
Clickworker\footnote{\url{http://www.clickworker.com}} or
Microworkers\footnote{\url{http://www.microworkers.com}} are the most flexible
of the three crowdsourcing system types, as they have the largest and most diverse
crowd of workers that can be directly accessed, and allow platform employers to
design and implement a large variety of tasks via web interface or API with
almost no restrictions. Typical tasks solved on crowd provider platforms are
image tagging, reviewing and correcting OCR results or transcribing audio to
text. Such tasks are easily solved using human intelligence but usually
impossible for a computer.

Of these three types of platforms we focus on the \emph{specialised} and on the
\emph{crowd provider} platforms. The aggregator platform can be seen as a
sort of ``meta'' platform, simply making good use of the latter two and
shielding the platform employer from the more intricate details of designing
and running a crowdsourcing initiative.

As discussed in secion~\ref{csdef} there must exist a clearly defined task for
a clearly defined crowd to solve. Tasks on specialised platforms can differ
greatly from tasks on crowd provider platforms. The former almost invariably
require creativity, expertise and time to solve, while the latter are mostly
repetitive and easily solved in mere seconds or minutes without much cognitive
effort.  These are called \emph{micro-tasks}.

The crowd's recompense for solving creative or R\&D tasks is usually vastly
greater than that for solving micro-tasks. In the case of InnoCentive the prize
for accepted solutions can be as high as \SI{100000}[\$]{} or in special cases
even \SI{1}[\$]{M}. In contrast the reward for solving a single micro
task is usually notably less than \SI{1}[\$]{}.

From the above it can be concluded that not every crowdsourcing initiative,
workflow or campaign can and will work on every platform, and that different
initiatives require different kinds of tasks, solved by different kinds of
crowds. 

\section{Motivating different crowds for different problems}

As discussed in the previous section it is obvious that different kinds of
problems attract different kinds of crowds. Thus the motivation and make-up of
crowds that solve creative or intellectual tasks is very different from crowds
that work on non-demanding micro-tasks. Questions about motivation and
composition of crowds are subject of ongoing
research~\cite{antikainen2010motivating, brabham2008moving, brabham2010moving,
	chandler2013breaking, horton2010labor, kaufmann2011more,
	mason2010financial}.

In the following paragraphs we discuss Brabham's analysis of motivation and
make-up of the crowds at
iStockphoto\footnote{\url{http://www.istockphoto.com}}~\cite{brabham2008moving}
and Threadless~\cite{brabham2010moving}, as two examples of crowdsourcing
initiatives involving highly creative tasks. 

As explained in~\cite{brabham2008moving} at iStockphoto the crowd, whose
members are called ``iStockers'', creates and uploads stock photographs,
animations or videos.  Clients find and download the stock they want, and
individual photographers make a small profit for each download, with
iStockphoto taking a portion of each profit. In accordance with the integrated
definition~\cite{estelles2012towards} discussed in section~\ref{csdef}
iStockphoto is indeed a crowdsourcing initiative: the clearly defined problem is
producing stock photography which is put into the form of an open call on the
internet, with solutions provided by a crowd of (amateur) photographers. Both
the crowd and the crowdsourcer profit from clients purchasing the created
stocks.

Brabham finds that the crowd at iStockphoto is elite and relatively homogenous,
and even though many nationalities are represented the dominant group consists
of middle- to upper-class, higher educated whites that are heavy internet users
with high-speed internet connections at home. Regarding their motivation he
finds that making money is the most important factor for members of the
iStockphoto crowd. Peer recognition, learning new skills and the joy of
participation are only secondary motivators. He notes that interestingly crowd
participants at iStockphoto are generally not interested in the online
community aspect and do not find their motivation in building a network of
friends and creative professionals through the site.

Analysing the crowd at Threadless in~\cite{brabham2010moving} Brabham comes to
similar, yet different conclusions. Threadless is ``an online t-shirt company
that crowdsources the design process for its shirts through an ongoing online
competition where registered members on the site can rate the design
submissions of fellow members on a five-point scale.''~\cite{brabham2010moving}
The highest-rated shirts are then produced and sold on the site, and the
winning designers are rewarded with \SI{2000}[\$]{} in cash and \SI{500}[\$] in
Threadless gift certificates. From this we can see that the integrated
definition of crowdsourcing~\cite{estelles2012towards} applies to Threadless as
well.

The biggest motivator at Threadless, Brabham finds, is the opportunity to make
money, much like at iStockphoto, with developing and honing creative skills and
leveraging freelance design work being secondary motivators. But at Threadless
the love of community is an important motivator that is missing at iStockphoto.
Another aspect unique to Threadless is that of crowd participants being
``addicted'' to the site and the community, meaning they see themselves as more
as meaningful actors in the company's business process than mere customers. It
is this kind of ``obsessive'' and vibrant community that makes Threadless such
an exemplary and successful crowdsourcing initiative.

In contrast to these creative crowdsourcing types stand micro-task-oriented
initiatives found at Amazon Mechanical Turk (AMT) and other crowd provider
platforms. In the following paragraphs we discuss research on crowd motivation
based on studies using AMT.

At AMT the crowd consists of over \num{100000} workers from over \num{100}
countries~\cite{buhrmester2011amazon} and is thus much larger and more diverse
than crowds at specialised platforms like discussed above. These workers solve
micro-tasks (so-called \emph{Human Intelligence Tasks}, or HITs) as posted by
requesters, for which they are paid upon completion.  The reward for each task
is set by the requester upon creation and is usually no more than a few cents.
Requesters may refuse to pay workers who submit low-quality work, which may
make workers with high refusal rates ineligible for participation in other
tasks. Amazon as crowd provider charges \num{10}\percent. Again the integrated
crowdsourcing definition~\cite{estelles2012towards} applies.

Research by Horton~\cite{horton2010labor}, Kaufmann~\cite{kaufmann2011more} and
Mason~\cite{mason2010financial} focuses mainly on how work output quality and quantity
vary when varying the financial reward for solving tasks,
while Chandler~\cite{chandler2013breaking} examines how the perceived
meaningfulness of a task affects quality and quantity of output.

According to Mason~\cite{mason2010financial} increasing payments for tasks
leads to increased quantity of work output, but not quality.
Horton~\cite{horton2010labor} finds complementary that workers work less when
compensation is lower but do not work less when the task is more time
consuming. 

The authors of~\cite{mason2010financial} also find that the design of the
compensation scheme (e.g., quota versus piece rate) can significantly affect
the quality of work output, to a point where better work can be produced for
less pay. This may even result in high-quality work for no financial
compensation, when the task's intrinsic motivation is big enough.

This conclusion is supported by Chandler~\cite{chandler2013breaking}. Here the
authors experimented with the perceived meaningfulness of tasks: assisting in
cancer research makes a task more meaningful, while knowing that the work
output will be discarded makes a task less meaningful. The authors find that
higher meaning increases quantity of work output (as well as quality, but only
insignificantly) and requires less compensation, and that, interestingly, tasks
with lower meaning lead to a decrease in quality of work output, but not in
quantity. 

From this discussion on the motivation and make-up of different crowds it can
be seen that the quality of work output is more easily recognized or confirmed
at crowdsourcing platforms that offer creative or intellectual tasks. They are,
by their nature, a meritocracy and it is easy to ``pick out a winner'' based on
the best-voted t-shirt design or the best-selling stock image. On the other
hand it is much more difficult to validate the quality of solutions of micro
tasks. Simply manually checking the entirety of every worker's output would
defeat the purpose of crowdsourcing work and also be infeasible, especially in
very large crowdsourcing initiatives. We discuss such challenges and possible
solutions in the next section.

% who makes up the crowd? what are their motivations?
% creative or intellectual problems vs repetitive and tedious problems
% community vs anonymity and micro-tasks
% amazon mturk vs innocentive/threadless
% importance or unimportance of heterogeneity of crowd
% hint at topic of quality control 

\section{Reliability in crowdsourcing initiatives}

In micro-task crowdsourcing initiatives it is usually not possible to quickly
and easily check the crowds work output for quality or correctness. This can
lead workers to ``game'' the system and solve micro-tasks in a random or
malicious fashion, with the intent to quickly produce as much work output as
possible (and getting paid for it) without investing any real effort. Such
behaviour has been observed in various experiments involving initiatives posted
to Amazon Mechanical Turk~\cite{hossfeld2013crowdtesting,
	kittur2008crowdsourcing, redi2013crowdsourcing}. The authors of these
experiments have devised recommendations on designing a crowdsourcing
initiative and its tasks in ways that limits cheating or makes it easier to
spot nonsensical work output.

In~\cite{kittur2008crowdsourcing} the authors crowdsourced the rating of
Wikipedia articles and found three ways to help eliminating most cheating or
malicious workers. First, the tasks should have some questions that are
objectively verifiable, such as how many references a given article cites.
Second, the tasks should be designed in a way that makes solving it honestly
require as much as or less effort than cheating. Third, there should be
multiple ways to spot suspect solutions that work even for subjective tasks,
such as suspiciously short solving times. The first way especially signalises
workers that their work output can and will be scrutinised and the third way
allows filtering obviously bogus work output early on.

Similar campaign design recommendations are found by the authors
of~\cite{hossfeld2013crowdtesting, redi2013crowdsourcing}, who engaged in user
studies concerning quality of experience which are highly subjective in
nature, such as rating images on recognisability and aesthetic appeal. They
find that a reliable crowd can be created by letting workers complete simple
and verifiable test tasks first and only allowing those that passed to
participate in the main task. Additionally the main task should contain
verification tests, such as captchas, consistency tests, such as ``In which
country do you live?'' and later ``On which continent do you live'', and content
questions about the task itself. The authors warn against adding too many such
tests and questions, lest workers quit the task in frustration. 

% kittur2008crowdsourcing, hossfeld2013crowdtesting
% what it takes to create a successful CS initiative
% Designing tasks, spreading tasks to many workers, testing workers, avoiding
% abuse/scams, ensuring quality work

\section{Critical considerations of crowdsourcing}

Crowdsourcing is viewed in quite a positive light in current literature which
often ignores potential or real drawbacks, and doesn't contain much in the way
of critical discussion of the practice. Thus, before concluding we present a
more critical view of crowdsourcing based on~\cite{simula2013rise}
and~\cite{brabham2008crowdsourcing}.

An often-cited pro-crowdsourcing argument is the diversity of the crowd and
that it contains participants from around the world~\cite{howe2006rise,
	howe2009crowdsourcing}. However, as Brabham finds 
in~\cite{brabham2008moving} the crowd at iStockphoto consists mostly of upper
or middle class, English speaking whites with higher education and high-speed
internet connections. This is probably especially representative of crowds at
other creative and intellectual crowdsourcing platforms, and Brabham states
``If solutions are measured against the yardstick of the company sponsoring the
crowdsourcing application, or measured against the opinions of the homogenous
crowd, alternatives to the presiding discourse will probably always lose out.''
Meaning the solutions provided by such a homogenous crowd are less
``disruptive'' and alternative because they originate from very similar
socio-economical backgrounds.

Additional problems with crowdsourcing include the discussed possibilities of
cheating, manipulation and gaming, as well as pranking and companies which
crowdsource tasks denying responsibility for failures by simply blaming
everything on ``the crowd.'' Another issue is that of labour exploitation.
Companies crowdsourcing designs or R\&D problems benefit hugely from clever
solutions by the crowd while the winning crowd participants are rewarded with
literally pennies on the dollar. 

It might also be argued that crowdsourcing may drive professionals (such as
stock photographers or designers) out of business, but that discussion about
capitalism is best left for economists and sociologists. In the end the real
winners of crowdsourcing are the companies running the crowd provider
platforms, such as Amazon or Clickworker, who profit from every problem posted
and every solution devised.

% simula2013rise, partly brabham2008moving
% manipulation, pranks, ``not invented here'', labour exploitation
% the ``crowd'' is not as diverse and heterogeneous as it might seem: white,
%   middle/upper class, English speaking, young, higher education, high-speed
%   internet connection
% the real winners in crowdsourcing are the platform providers, profiting from every problem and every solution


\section{Conclusion}

In this paper we provided a short overview of the practice of crowdsourcing
based on current literature and its often inconsistent definitions. We reviewed
the differences between creative/intellectual crowdsourcing initiatives and
those based on micro-tasks requiring very little cognitive effort, and
discussed research on the motivation and make-up of different crowds solving
different problems. We also examined the recommendations authors of
crowdsourcing experiments provide for designing a crowdsourcing campaign that
is not easily cheated or gamed and closed with a short discourse on some
critical consideration of the crowdsourcing practice which are often ignored in
the literature.

%\nocite{*}
\bibliographystyle{abbrv}
\bibliography{crowdsourcing}

\end{document}
